Title:
Neural Collapse on Graph Transformers

Abstract:
 Building upon recent theoretical and empirical investigations on neural collapse in Message-Passing Neural Networks (MPNNs), we extend this analysis to a wider variety of graph learning models, particularly graph transformers. Prior work established that MPNNs exhibit variability collapse when trained over synthetic data, and identified a structural constraint on the input graph necessary for collapse in a theoretical setting. In our work, we explore the degree of neural collapse in graph transformers, both on synthetic and real-world data. Our findings indicate the graph transformers exhibit similar collapse behavior in all tested settings, despite substantial differences in model architecture.
